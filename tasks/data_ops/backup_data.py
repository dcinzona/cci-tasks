from pathlib import Path
import csv
import copy
import time
import typing as T

from tasks.data_ops.filterable_objects import (
    OPT_IN_ONLY,
)

from tasks.data_ops.generate_extract_mapping import GenerateExtractMapping
from tasks.data_ops.get_sobjects import GetSObjects
from tasks.data_ops.extract_dataset_utils.synthesize_extract_declarations import (
    flatten_declarations,
)

from cumulusci.core.datasets import _make_task  # , Dataset
from cumulusci.tasks.bulkdata.extract import ExtractData
from cumulusci.tasks.bulkdata.mapping_parser import MappingSteps
from cumulusci.tasks.salesforce.SOQLQuery import SOQLQuery
from cumulusci.tasks.salesforce.BaseSalesforceApiTask import BaseSalesforceApiTask
from cumulusci.tasks.bulkdata.generate_mapping_utils.generate_mapping_from_declarations import (
    SimplifiedExtractDeclarationWithLookups,
    classify_and_filter_lookups,
)
from cumulusci.core.exceptions import BulkDataException
from cumulusci.salesforce_api.org_schema import Filters, get_org_schema
from cumulusci.tasks.bulkdata.extract_dataset_utils.extract_yml import (
    ExtractRulesFile,
    ExtractDeclaration,
)
from cumulusci.tasks.bulkdata.mapping_parser import validate_and_inject_mapping
from cumulusci.tasks.bulkdata.step import (
    DataOperationType,
    DataOperationStatus,
    get_query_operation,
)
from cumulusci.core.exceptions import TaskOptionsError
from cumulusci.salesforce_api.org_schema import Schema
from cumulusci.core.utils import process_bool_arg, process_list_arg
from cumulusci.utils import log_progress
from tasks.data_ops.overrides import init_overrides

extract_data_options = copy.deepcopy(ExtractData.task_options)
extract_data_options["mapping"]["required"] = False  # this will be generated by capture
soql_query_options = copy.deepcopy(SOQLQuery.task_options)


class BackupData(BaseSalesforceApiTask):
    """
    Task to backup data from a Salesforce org to a local directory.
    Example: cci task run backup_data --org <org_alias> --extraction-defition datasets/extract_accounts.yml
    """

    task_options = {
        "extraction_definition": {
            "description": "The path to the extraction definition file. File will be created if it does not exist",
            "required": False,
        },
        "preview": {
            "description": "Preview the data extraction without writing to disk. Default is False",
            "required": False,
        },
        "execute": {
            "description": "Perform the full extract. If not set, only the mapping file is generated. Default is False",
            "required": False,
        },
        "include_setup_data": {
            "description": "Include setup data like ApexClasses. Default is False",
            "required": False,
        },
        "sobjects": {
            "description": "A comma separated list of sobjects to extract.  Overrides the extraction definition and includes all fields",
            "required": False,
        },
        "populated_only": {
            "description": "Only include objects with records. Default is True",
            "required": False,
        },
        "include_children": {
            "description": "Include child references in the extraction. By default, we relate only to the parents of the objects defined",
            "required": False,
        },
        "exclude_referenced": {
            "description": "Exclude sObjects defined by lookup / reference fields from the extraction. Default is False",
            "required": False,
        },
        "include_files": {
            "description": "Include files in the extraction. Default is False",
            "required": False,
        },
    }

    always_include_objects = ["User", "Group"]
    unix_time = int(time.time())

    def _init_options(self, kwargs):
        self.logger.info(init_overrides())
        super()._init_options(kwargs)
        self.orgname = self.org_config.name
        self.preview = process_bool_arg(self.options.get("preview", True))
        self.execute = process_bool_arg(self.options.get("execute", False))
        self.include_setup_data = process_bool_arg(
            self.options.get("include_setup_data", False)
        )
        self.sobjects = process_list_arg(self.options.get("sobjects"))
        self.root_dir = Path(self.project_config.repo_root or "")
        self.populated_only = process_bool_arg(
            self.options.get("populated_only", False)
        )
        self.include_files = process_bool_arg(self.options.get("include_files", False))

        user_provided_def = self.options.get("extraction_definition")
        if user_provided_def:
            self.extraction_definition = Path(user_provided_def)
            assert self.extraction_definition.exists(), f"File not found: {self.extraction_definition}"

        self.include_child_references = process_bool_arg(
            self.options.get("include_children")
        )
        self.exclude_referenced_objects = process_bool_arg(
            self.options.get("exclude_referenced")
        )
        if self.exclude_referenced_objects:
            if self.sobjects is not None and len(self.sobjects) > 0:
                self.always_include_objects = []
                self.include_child_references = False
            else:
                self.exclude_referenced_objects = False
                raise TaskOptionsError(
                    "Excluding referenced objects requires a list of sobjects to be specified."
                )

    @property
    def path(self) -> Path:
        return self.root_dir / "datasets" / self.orgname / "extracts"

    @property
    def data_path(self) -> Path:
        return self.path / f"{self.unix_time}"

    @property
    def extract_file(self) -> Path:
        return self.path / f"{self.unix_time}.extract-definition.yml"

    @property
    def mapping_file(self) -> Path:
        return self.path / f"{self.unix_time}" / f"{self.unix_time}.mapping.yml"

    def _csv_path(self, sobject):
        return self.data_path / f"{sobject}.csv"

    def _run_task(self):
        self._run_extract()
        list_todo(self.logger)

    def _run_extract(self):
        getObjectsTask = _make_task(
            GetSObjects,
            project_config=self.project_config,
            org_config=self.org_config,
            include_tooling=self.include_setup_data,
            filters="retrieveable,updateable",
        )
        self.valid_objects = getObjectsTask()
        self.valid_object_names = set({f["name"] for f in self.valid_objects})

        filters = [Filters.retrieveable, Filters.updateable]
        if self.populated_only:
            filters.append(Filters.populated)

        with get_org_schema(
            self.sf,
            self.org_config,
            include_counts=True,
            included_objects=self.valid_object_names,
            filters=filters,
        ) as schema:
            if not self.data_path.exists():
                self.data_path.mkdir(parents=True, exist_ok=True)

            self.logger.info(f"Valid objects: {len(self.valid_object_names)}")
            self.logger.info(f"Schema objects: {len(schema.keys())}")
            if len(self.valid_object_names) != len(schema.keys()):
                # get the difference
                difference = self.valid_object_names.difference(schema.keys())
                self.logger.info(
                    f"Differece: {difference} ({len(difference)} objects)"
                )

            self.logger.info(f"Extraction directory: {self.data_path}")
            self.mapping = self._build_decls_input(
                schema
            )  # checks the sobjects input and adds all fields

            sobjectArray = list(self.mapping.keys())
            self.logger.info(f"...Identified {len(sobjectArray)} objects to process")

            if len(sobjectArray) < 5:
                self.logger.info(f"Objects: {sobjectArray}")
            include = (
                ",".join(list(set(sobjectArray)))
            )
            deltaignore = {o for o in self.valid_object_names if o not in sobjectArray}
            ignore = ",".join(deltaignore)

            assert not any([f in sobjectArray for f in deltaignore])

            self._build_mapping(include, ignore)

            if self.execute:
                self.logger.info(
                    f"\n...Extracting data for {len(self.mapping.keys())} objects..."
                )
                for mapping in self.mapping.values():  # self.mapping.values():
                    soql = self._soql_for_mapping(mapping)
                    self._run_query(soql, mapping)

                if self.include_files:
                    self.logger.info("...Extracting files")

                    from cumulusci.tasks.salesforce.salesforce_files import RetrieveFiles
                    retrieveFilesTask = _make_task(
                        RetrieveFiles,
                        project_config=self.project_config,
                        org_config=self.org_config,
                        path=self.data_path / "Files",
                    )
                    retrieveFilesTask()
            # self._extract_files():

            self.print_summary()

    def _build_decls_input(self, schema: Schema):
        """Build the input declarations for the extract process (this will also explode group declarations)"""
        self.logger.info("...Building extract declarations")
        decls = {}
        if self.sobjects:
            for sf_object in self.sobjects:
                if sf_object not in self.valid_object_names:
                    self.logger.warning(
                        f"Skipping {sf_object} as it is not a valid object"
                    )
                    continue
                decls[sf_object] = ExtractDeclaration(sf_object=sf_object)
        else:
            self.logger.info(
                f"\n...Processing extraction definition: {self.extraction_definition}"
            )
            decls = ExtractRulesFile.parse_extract(self.extraction_definition)

        standard_objects = {f["name"] for f in self.valid_objects if f["custom"] is False}
        custom_objects = {f["name"] for f in self.valid_objects if f["custom"] is True}

        decls_to_check = decls.copy()

        for k, d in decls_to_check.items():
            if d.is_group:
                decl = decls.pop(k)
                self.logger.info(f"...Expanding group declaration: {decl.sf_object}")
                gt = decl.group_type.upper()
                if gt == "STANDARD":
                    decls.update(
                        {
                            obj: ExtractDeclaration(sf_object=obj, fields=["FIELDS(ALL)"])
                            for obj in standard_objects
                        }
                    )
                elif gt == "CUSTOM":
                    decls.update(
                        {
                            obj: ExtractDeclaration(sf_object=obj, fields=["FIELDS(ALL)"])
                            for obj in custom_objects
                        }
                    )
                elif gt == "ALL":
                    decls.update(
                        {
                            obj: ExtractDeclaration(sf_object=obj, fields=["FIELDS(ALL)"])
                            for obj in self.valid_object_names
                        }
                    )
                else:
                    raise TaskOptionsError(
                        f"Invalid group type: {decl.group_type}. Valid values are: STANDARD, CUSTOM, ALL"
                    )
            elif d.sf_object not in self.valid_object_names:
                self.logger.warning(
                    f"Skipping {d.sf_object} as it is not a valid object"
                )
                decls.pop(k)
                continue

        if self.include_child_references:
            decls = self.include_referencing_objects(decls, schema, OPT_IN_ONLY)

        if len(decls.keys()) == len(self.valid_object_names):
            self.logger.info("   (All objects are included)")
            return dict(pair for pair in decls.items())

        # explode group declarations
        self.logger.info("...flatten_declarations")
        simplified_decls = flatten_declarations(decls.values(), schema, OPT_IN_ONLY)
        self.logger.info(
            f"   {len(simplified_decls)} declarations after flatten_declarations"
        )
        simplified_decls = classify_and_filter_lookups(simplified_decls, schema)
        self.logger.info(
            f"   {len(simplified_decls)} declarations after classify_and_filter_lookups"
        )
        self.decls = simplified_decls
        mappings = [
            self._mapping_decl_for_extract_decl(decl) for decl in simplified_decls
        ]
        self.logger.info(f"   {len(mappings)} mappings after _mapping_decl_for_extract_decl")
        return dict(pair for pair in mappings if pair)

    def print_summary(self):
        self.logger.info(" *** No data will be extracted. *** \n") if not self.execute else None
        lb = "-" * 80
        lb = f"\n{lb}\n"

        self.logger.info(f"\nSUMMARY{lb}")

        self.logger.info("*** Preview mode enabled. No data was be extracted. ***\n")
        sorted_mapping = [
            f"{k['sf_object']} ({len(k['fields'])} {'Fields' if len(k['fields']) > 1 else 'Field'})"
            for k in self.mapping.values()
        ]
        sorted_mapping.sort()
        deli = "\n - "
        if len(self.mapping.keys()) < 40:
            self.logger.info(f"sObjects: {deli}{deli.join(sorted_mapping)}")
        else:
            self.logger.info(
                f"sObjects: {deli}{deli.join(sorted_mapping[:39])}\n... (output truncated){deli}{sorted_mapping[-1]}"
            )

        self.logger.info(f"\nNumber of sObjects identified: {len(self.mapping.keys())}")
        if self.include_files:
            available_files = [
                {
                    "Id": result["Id"],
                    "FileName": result["Title"],
                    "FileType": result["FileType"],
                    "VersionData": result["VersionData"],
                    "ContentDocumentId": result["ContentDocumentId"],
                }
                for result in self.sf.query(
                    "SELECT Title, Id, FileType, VersionData, ContentDocumentId FROM ContentVersion WHERE isLatest=true "
                )["records"]
            ]

            self.logger.info(f"Number of files found: {len(available_files)}")

    def _build_mapping(self, include: str, ignore: str):
        self.logger.info(
            "...Getting related objects and building mapping file for extract"
        )
        mappingTask = _make_task(
            GenerateExtractMapping,
            project_config=self.project_config,
            org_config=self.org_config,
            logger=self.logger,
            path=self.mapping_file,
            include=include,
            ignore=ignore,
            break_cycles="auto",
            exclude_setup_objects=not self.include_setup_data,
        )
        mappingTask()
        self.logger.info(f"Mapping saved to : {self.mapping_file}")
        self.mapping = MappingSteps.parse_from_yaml(self.mapping_file)

    def _run_query(self, soql, mapping):
        """Execute a Bulk or REST API query job and store the results."""
        csvPath = self._csv_path(mapping["sf_object"])
        field_map = mapping.get_complete_field_map(include_id=True)
        columns = [field_map[f] for f in field_map]

        self.logger.info(f"Querying {mapping['sf_object']}")

        with open(csvPath, "w") as f:
            f.write(",".join(columns))
            f.write("\n")

        step = get_query_operation(
            sobject=mapping.sf_object,
            api=mapping.api,
            fields=list(mapping.get_extract_field_list()),
            api_options={},
            context=self,
            query=soql,
        )

        step.query()

        if step.job_result.status is DataOperationStatus.SUCCESS:
            if step.job_result.records_processed:
                self.logger.info("Downloading and importing records")
                self._process_results(mapping, step, csvPath=csvPath)
            # else:
            #     self.logger.info(f"No records found for sObject {mapping['sf_object']}")
        else:
            self.logger.error(f"Error querying {mapping['sf_object']}")
            self.logger.error(f"SOQL Query: {soql}")
            raise BulkDataException(
                f"Unable to execute query: {','.join(step.job_result.job_errors)}"
            )

    def _process_results(self, mapping, step, csvPath):
        """Process the results of a query and write them to a CSV file."""
        record_iterator = log_progress(step.get_results(), self.logger)
        writer = csv.writer(open(csvPath, "a"), quoting=csv.QUOTE_ALL)
        for row in record_iterator:
            writer.writerow(row)

    def _soql_for_mapping(self, mapping):
        """Return a SOQL query suitable for extracting data for this mapping."""
        sf_object = mapping.sf_object
        fields = mapping.get_extract_field_list()
        soql = f"SELECT {', '.join(fields)} FROM {sf_object}"

        if mapping.record_type:
            soql += f" WHERE RecordType.DeveloperName = '{mapping.record_type}'"

        if mapping.soql_filter is not None:
            soql = self.append_filter_clause(
                soql=soql, filter_clause=mapping.soql_filter
            )

        return soql

    def include_referencing_objects(
        self,
        decls: T.Dict[str, ExtractDeclaration],
        schema: Schema,
        opt_in_only: T.Sequence[str],
    ):
        """Include objects that have lookups to mapped objects"""
        self.logger.info("...Populating child references")

        if len(decls.keys()) == len(self.valid_object_names):
            self.logger.info("   (All objects are already included)...skipping")
            return decls  # all objects are already included

        base_objects = {
            decl
            for decl in decls.keys()
            if "(" not in decl and decl in self.valid_object_names
        }

        if not base_objects:
            self.logger.info("No base objects to reference")
            return decls

        self.logger.info(
            f"Getting objects that reference object declarations: \n{base_objects}"
        )
        # loop through all valid objects and check if they reference any of the mapped objects
        referenced_objects = []
        for obj in self.valid_object_names:
            if (
                obj in base_objects  # already included
                or obj in decls.keys()  # already included
            ):
                continue

            reference_fields = [
                f
                for f in schema[obj].fields.values()
                if f["type"] == "reference"
                and any(
                    [
                        x
                        for x in f["referenceTo"]
                        if x
                        not in (
                            "User",
                            "Group",
                            "RecordType",
                        ) + OPT_IN_ONLY  # everything looks up to these objects so we exclude them
                        and x in self.valid_object_names  # make sure we can retrieve the target object
                        and (
                            x in base_objects  # or x in mapped_objects
                        )  # everything looks up to these objects so we exclude them
                    ]
                )
            ]

            for field in reference_fields:
                if obj not in base_objects and obj not in referenced_objects and obj not in OPT_IN_ONLY:
                    referenced_objects.append(obj)
                    decls[obj] = ExtractDeclaration(
                        sf_object=obj
                    )
                    parent_objects = [x for x in field["referenceTo"] if x in base_objects]
                    self.logger.info(
                        f"Adding {obj} because {obj}.{field['name']} references {parent_objects}"
                    )

        self.logger.info(
            f"\nAdded {len(decls) - len(base_objects)} objects that reference mapped objects"
        )

        return decls

    def _mapping_decl_for_extract_decl(
        self,
        decl: SimplifiedExtractDeclarationWithLookups,
    ):
        """Make a CCI extract mapping step from a SimplifiedExtractDeclarationWithLookups"""
        lookups = {lookup: {"table": tables} for lookup, tables in decl.lookups.items()}
        mapping_dict: dict[str, T.Any] = {
            "sf_object": decl.sf_object,
        }
        if decl.where:
            mapping_dict["soql_filter"] = decl.where
        if decl.api:
            mapping_dict["api"] = decl.api.value
        mapping_dict["fields"] = decl.fields
        if lookups:
            mapping_dict["lookups"] = lookups

        return (decl.sf_object, mapping_dict)


class ExtractBackup(ExtractData):

    task_options = extract_data_options

    def _init_options(self, kwargs):
        super()._init_options(kwargs)
        self.mapping = None

    def _run_task(self):
        init_overrides()
        self._init_mapping()
        with self._init_db():
            for mapping in self.mapping.values():
                soql = self._soql_for_mapping(mapping)
                self._run_query(soql, mapping)

            self._map_autopks()

            if self.options.get("sql_path"):
                self._sqlite_dump()

    def _init_mapping(self):
        """Load a YAML mapping file."""
        mapping_file_path = self.options["mapping"]
        if not mapping_file_path:
            raise TaskOptionsError("Mapping file path required")
        self.logger.info(f"Mapping file: {self.options['mapping']}")

        self.mapping = MappingSteps.parse_from_yaml(mapping_file_path)

        validate_and_inject_mapping(
            mapping=self.mapping,
            sf=self.sf,
            namespace=self.project_config.project__package__namespace,
            data_operation=DataOperationType.QUERY,
            inject_namespaces=self.options["inject_namespaces"],
            drop_missing=self.options["drop_missing_schema"],
            org_has_person_accounts_enabled=self.org_config.is_person_accounts_enabled,
        )

    def _map_autopks(self):
        # Convert Salesforce Ids to autopks
        for m in self.mapping.values():
            lookup_keys = list(m.lookups.keys())
            if not m.get_oid_as_pk():
                if lookup_keys:
                    self._convert_lookups_to_id(m, lookup_keys)


def list_todo(logger):

    todolist = [
        "Add database support",
        "Add support for PKI encryption of extracted data",
    ]

    lb = "-" * 80
    lb = f"\n{lb}\n"
    deli = "\n - "

    logger.info(f"{lb}TODO:{lb} - {deli.join(todolist)}")
